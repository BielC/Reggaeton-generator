{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Lyrics generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BielC/Reggaeton-generator/blob/master/Lyrics_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26r8X3lGpNDh",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning Final Project - Lyrics Generation\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "    Biel Casals - 206743\n",
        "    Aran Coll - 204887\n",
        "    Gabriel Graells - 205638\n",
        "\n",
        "---\n",
        "For this project, we implement a RNN capable of generating new song lyrics character by character after being trained with hundreds of reggeaton songs of the best singers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgTUyj4YpNDk",
        "colab_type": "text"
      },
      "source": [
        "For this project we'll be using numpy and torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-B3Y6rvpNDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g01ZIWGosL3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a10b280e-73bd-4715-e281-043ebda399a4"
      },
      "source": [
        "#mount Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2aOj4NCrtJx",
        "colab_type": "text"
      },
      "source": [
        "# Data Collection\n",
        "In order to generated the needed data set for training we have used [this](https://github.com/johnwmillr/LyricsGenius) library that has been build on top of the API of the music content website [Genius](https://genius.com/).\n",
        "The code searches for the top 50 songs, based on popularity, of the artist in the list, then downloads one JSON file per artist containing the songs and further information. Finally it transfers all songs from the JSONs file to the output file \"*lyrics.txt*\".\n",
        "\n",
        "**If you want to execute this part of the code is better to do it locally in a *.py* Python script.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz3ud1cfrzXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install lyricsgenius\n",
        "import lyricsgenius\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "artist_list = ['Bad Bunny', 'Daddy Yanky','Ozuna', 'Don Omar', 'Anuel AA','Maluma','Juan Magan','Nicky Jam', 'Karol G','Mozart La Para', 'Cosculluela', 'Tito \"El Bambino\"','Calle Trece', 'Arcangel','Tego Calderon','Plan B']\n",
        "\n",
        "#Init API client\n",
        "genius = lyricsgenius.Genius(\"XAjzWCsgJw9mHU48O7RRZ6-nzXykKtYH9_7zAFnPbl_PHrVQZQBM7InGU05sji9o\")\n",
        "genius.verbose = False \n",
        "genius.remove_section_headers = True\n",
        "genius.excluded_terms = ['Live', '(Live)']\n",
        "\n",
        "#Iterate artist_list and download JSON file with lyrics\n",
        "for art in artist_list:\n",
        "    print(art)\n",
        "    artist = genius.search_artist(art, max_songs = 50, sort='popularity')\n",
        "    \n",
        "    if artist == None:\n",
        "        print(art,'Does not exist!')\n",
        "    else:\n",
        "        print('saving json...')\n",
        "        artist.save_lyrics()\n",
        "\n",
        "#Get names json files in current directory\n",
        "json_files = [pos_json for pos_json in os.listdir() if pos_json.endswith('.json')]\n",
        "\n",
        "for files in json_files:\n",
        "    with open(files) as json_file, open('lyrics.txt', 'a') as text_file:\n",
        "        data = json.load(json_file)\n",
        "        for s in data['songs']:  \n",
        "            text_file.write(s['lyrics'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W6yVnS2r15T",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning\n",
        "After the \"lyrics.txt\" file is generated we need to review the downloaded data and define a criteria to clean our it with the goal of improving the network performance.\n",
        "The model for our lyrics generator learns on a char level, meaning it uses a dictionary of characters and learns which character is the more likely to follow a given \n",
        "sequence. Thus reducing the amount of characters will \n",
        "reduce the decision range and the complexity of the problem.\n",
        "\n",
        "Initially our data contained a total of 131 characters, we printed them to inspect them and we found Korean and Germanm letters. For this cases we took the extreme approach of deleting the songs containing those characters. \n",
        "\n",
        "Still, there where remaining a series of simbols in the character dictionary which we deleted them with the following code snipped:\n",
        "```\n",
        "f1 = open('lyrics.txt', 'r')\n",
        "f2 = open('lyrics.txt', 'w')\n",
        "for line in f1:\n",
        "    f2.write(line.replace(, ))\n",
        "f1.close()\n",
        "f2.close()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz_TMlzIr67g",
        "colab_type": "text"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjX3OaIDtXi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('/content/drive/My Drive/DeepLearning_2020/FP/lyrics.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8s3iGNvpND6",
        "colab_type": "text"
      },
      "source": [
        "We check the first 100 characters of our dataset to make sure the format is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVxtITJmpND7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "641a1c67-f66f-4dd7-d11a-c63fef27fcb7"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"---\\nY esta noche está pa' bailar, beber, jode'\\nHasta que no pueda más (Yeh-yeh-yeh-yeh)\\nY esta noche\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1eOqE8rpNEB",
        "colab_type": "text"
      },
      "source": [
        "# Data Encoding\n",
        "A neural neural model needs numerical inputs for a better performance so we tranform each char into an interger by simply enumerating all the unique characters in out lyrics file. We generate two dictionaries, ***int2char*** maps an integer value to the associated character and ***char2int*** does the inverse mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xRBEK92pNEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars)) #maps integers to characters\n",
        "char2int = {ch: ii for ii, ch in int2char.items()} #maps characters to unique integers\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1jZaimvpNEI",
        "colab_type": "text"
      },
      "source": [
        "And we can see those same characters from above, encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkKDujngpNEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "0001752c-8347-4e03-af51-c14ca4821aa2"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([73, 73, 73,  1, 65, 29, 53, 90, 67, 48, 29, 52, 79, 16, 19, 53, 29,\n",
              "       53, 90, 67, 25, 29, 55, 48, 77, 29, 64, 48, 87, 14, 48, 94, 44, 29,\n",
              "       64, 53, 64, 53, 94, 44, 29, 31, 79, 41, 53, 77,  1, 35, 48, 90, 67,\n",
              "       48, 29, 26, 32, 53, 29, 52, 79, 29, 55, 32, 53, 41, 48, 29, 15, 25,\n",
              "       90, 29, 83, 65, 53, 19, 73, 20, 53, 19, 73, 20, 53, 19, 73, 20, 53,\n",
              "       19, 62,  1, 65, 29, 53, 90, 67, 48, 29, 52, 79, 16, 19, 53])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCCpJfP3pNER",
        "colab_type": "text"
      },
      "source": [
        "## One Hot Encoding\n",
        "\n",
        "As stated before is is mandatory to transform the character to a numerical representation. \n",
        "\n",
        "One hot encoding is a commom practice for categorical data and for text generation.\n",
        "\n",
        "One Hot encoding consists on representing each character as a binary vector of N dimensions, where N is the number of categories (characters) in the data, it would have a 1 in the position associated to that character and 0 in the rest of positions.\n",
        "\n",
        "If we have a dictionary with three characters [a, t, c] and we want to encode the word \"cat\" it would be represent it as follows:\n",
        "\n",
        "[[0 0 1]\n",
        "[1 0 0]\n",
        "[0 1 0]].\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Why One Hot Encoding?**\n",
        "\n",
        "Even though we could use Interger Encoding One Hot Encoding preserves the independence between categories outputing much better results.\n",
        "\n",
        "Integer encoding introduces a natural continuity between categories, assume we map value 2 with letter 'a' and 3 to letter 't'. There is a natural numerical continuity between 2 and 3, so if the actual output **y** should be 3 and the model guesses 2.49 the answer is close enought to our target. If we translate this logic to characters we can appreciate that there is no actual continuity between letter 'a' to letter 't'.\n",
        "\n",
        "One Hot encoding brakes this continuity by generating N buckets (one for each category), the model gives \"points\" to a bucket if the associated character is likely to follow the senquence. The char with more \"points\" in its bucket is the one selected to follow the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "001we_cipNET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the correct index with a one\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kimc1gaNpNEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "5b663afa-37fd-4ae5-e3d5-3fd7e3de328b"
      },
      "source": [
        "# check the one_hot_encoder function\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk-KJQ9opNEf",
        "colab_type": "text"
      },
      "source": [
        "## Making training mini-batches\n",
        "\n",
        "We'll take the encoded characters and split them into multiple sequences, given by `batch_size`. Each of our sequences will be `seq_length` long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-XmRfX1pNEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # Get the number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    \n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # Iterate over the batches using a window of size seq_length\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # targets\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faOhnpR1pNEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing the function with batch size=8 and sequence length=50\n",
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpJ-UE_JpNEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "e16c4604-97ce-4a75-e740-c33480ccfa7c"
      },
      "source": [
        "# Print the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[73 73 73  1 65 29 53 90 67 48]\n",
            " [39 48 41 48 29 13 53 18 29 26]\n",
            " [53  0 32 94 79 29 53 52 29 15]\n",
            " [29 53 14 29 16 48 90 53 94 82]\n",
            " [90 53 29 26 32 53 41 48 29 90]\n",
            " [26 32 53 29 15 53 29 19 87 16]\n",
            " [48 29 55 87 14 48  1 96 48 29]\n",
            " [48 14  1 96 94 94 94 48 48  1]]\n",
            "\n",
            "y\n",
            " [[73 73  1 65 29 53 90 67 48 29]\n",
            " [48 41 48 29 13 53 18 29 26 32]\n",
            " [ 0 32 94 79 29 53 52 29 15 87]\n",
            " [53 14 29 16 48 90 53 94 82 79]\n",
            " [53 29 26 32 53 41 48 29 90 79]\n",
            " [32 53 29 15 53 29 19 87 16 87]\n",
            " [29 55 87 14 48  1 96 48 29 26]\n",
            " [14  1 96 94 94 94 48 48  1 35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P46dmpXxpNE1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Defining the network\n",
        "Our model is composed by three layers:\n",
        "* An LSTM layer\n",
        "* A dropout layer\n",
        "* A fully-connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8KqBg2XpNE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7f0e3de-b57d-4722-edb4-ee7be8ee97f2"
      },
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "  print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9ZnkvYKpNFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # define the layers of the model\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "      \n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs \n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikvPvR-5pNFH",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "We're using an Adam optimizer and cross entropy loss since we are looking at character class scores as output.\n",
        "\n",
        "We use gradient clipping to help prevent exploding gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fQ1wnVVpNFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            \n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # we use the `clipping_gradient_norm` method to prevent the exploding gradient problem\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCqRtccepNFO",
        "colab_type": "text"
      },
      "source": [
        "## Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-n2n1OnpNFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_hidden= 1024\n",
        "n_layers= 3\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EFNKXP1XpNFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "4f717dad-53f7-4967-dcb2-b35b79413f3e"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 10\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.0001, print_every=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 100... Loss: 3.2228... Val Loss: 3.2246\n",
            "Epoch: 2/10... Step: 200... Loss: 3.2268... Val Loss: 3.2218\n",
            "Epoch: 3/10... Step: 300... Loss: 3.2381... Val Loss: 3.2181\n",
            "Epoch: 4/10... Step: 400... Loss: 3.2147... Val Loss: 3.2003\n",
            "Epoch: 5/10... Step: 500... Loss: 3.1125... Val Loss: 3.0942\n",
            "Epoch: 6/10... Step: 600... Loss: 2.7845... Val Loss: 2.7630\n",
            "Epoch: 7/10... Step: 700... Loss: 2.7267... Val Loss: 2.6221\n",
            "Epoch: 8/10... Step: 800... Loss: 2.4873... Val Loss: 2.4605\n",
            "Epoch: 9/10... Step: 900... Loss: 2.4134... Val Loss: 2.3729\n",
            "Epoch: 10/10... Step: 1000... Loss: 2.3429... Val Loss: 2.3160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkCTuv_8pNFc",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Here are the hyperparameters for the network.\n",
        "\n",
        "In defining the model:\n",
        "* `n_hidden` - The number of units in the hidden layers.\n",
        "* `n_layers` - Number of hidden LSTM layers to use.\n",
        "\n",
        "We assume that dropout probability and learning rate will be kept at the default, in this example.\n",
        "\n",
        "And in training:\n",
        "* `batch_size` - Number of sequences running through the network in one pass.\n",
        "* `seq_length` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
        "* `lr` - Learning rate for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs_YYfNUpNFd",
        "colab_type": "text"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAOXx5NEpNFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = f'rnn_{n_epochs}_epoch.pt'\n",
        "results_path = '/content/drive/My Drive/DeepLearning_2020/FP/Results/'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "torch.save(checkpoint, results_path + model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl5X5abUpNGS",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Sampling\n",
        "\n",
        "With the trained model, we can sample by passing in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTigLVKipNGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QulPF9MhpNGj",
        "colab_type": "text"
      },
      "source": [
        "We use a prime to build up a hidden state, otherwise the network will start out generating characters at random. The characters will be less coherent since it hasn't built up a long history of characters to predict from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M2P2wBNpNGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='Baila', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxQVR7svpNHE",
        "colab_type": "text"
      },
      "source": [
        "## Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqCg5pBXpNHG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "503bd12b-4448-404b-c0b7-fd23bbf6e2af"
      },
      "source": [
        "# Here we have loaded in a model that trained over 100 epochs `rnn_100_epoch.net`\n",
        "with open('/content/drive/My Drive/DeepLearning_2020/FP/Results/rnn_100_epoch.ckpt', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVhwdynbpNHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "outputId": "f0c3d522-be9a-456b-8ef4-c740f5ba07d5"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=2, prime=\"Baila\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baila me dicen que tú no la abajo yo soy tuyo\n",
            "Y tú sabes que yo no soy tu camino al cielo, y a ti no voy a estar no hay\n",
            "Yo no te voy a enla mal\n",
            "Y tú está' buscando en la mañana (Yah)\n",
            "Y ahora todos to' si me puse pa' mi amor\n",
            "Y yo te quiero a ti (pa' no ah)\n",
            "Por eso me dice la loca, también (Tú no va' a volver)\n",
            "Y ya no te me apare' como tú no me así'\n",
            "Ya me paso como un tiempo pa' comprarte (Yeah)\n",
            "Y tú ere' una diabla y te vas a ver, eh-eh (Uah)\n",
            "Y todo el amor soy en el camino (Yeah)\n",
            "\n",
            "Yo sé que está canción (-mande), para mí maminarto (Yeah)\n",
            "Y ahora to' todo lo que te hace (Nada)\n",
            "Porque tú no vive' igual que yo (Tú no vive' igual), ah-ah\n",
            "\n",
            "Te doba te dar (Yeh-yeh)\n",
            "Baby, tú nadie ve' en el pela'o (Yeah, yaa')\n",
            "\n",
            "Yo sé que tú me desea' (Que vayan a mirarme)\n",
            "Tú no vive' igual que yo (Y yo no tengo miedo (No)\n",
            "Ya no sé qué tú quiere', ya no sé pa' dónde ve (Yeah)\n",
            "Y aunque tú te va' a dar (Ah)\n",
            "Y te vo'a matar (-ah)\n",
            "Yo te acorro de mí (Dama)\n",
            "Porque tú me dices que yo te voy a decir que ya no ere' tú y yo\n",
            "\n",
            "Entre tu y yo, es que yo soy un hombre tus ojos (Wuh)\n",
            "A ti nadie te queda la noche (No te deseas en el camano)\n",
            "Y aunque te vista me dice \"Que en la pasa y me escapas (Yeh)\n",
            "Yo te amo, yo te amo\n",
            "Y aunque tú tienes la olora\n",
            "Y aunque yo te ponga bandarte a ti\n",
            "Que no me impocta lo que pasa nada (No)\n",
            "No quiero que me perdones\n",
            "Y no le digas que no\n",
            "Si te pillo por ti\n",
            "Y tú no me queras, para mí\n",
            "\n",
            "Yo te llevo a ti, y a ti te mato mama\n",
            "Y tú mami cuando te pegas (Yeh-yeh-yeh-yeh)\n",
            "Y yo te quiero amarte\n",
            "Y aunque tú te viele tocar (Woh-oh-oh)\n",
            "Y yo te quiero comprar (Yeh-eh)\n",
            "Y tú me dice' yo te pongo a tu mejar (Baby)\n",
            "Y te vo'a mal de manira, yo no sé por qué siempre me la encanta (Oh-oh-oh)\n",
            "Y tú matarme en casa, mana mi corazón\n",
            "Y ahora que te tenga a tu cama, ey, ey\n",
            "\n",
            "Yo sé que tú tienes malo, me arraba el amor\n",
            "Y a mí mami me hace dejar la corazón\n",
            "Y ahora solo te ves, me hace con otra (De mí)\n",
            "\n",
            "De tu cuerpo y tú me dicientos (Nah)\n",
            "Y yo no sé como qué, pasó\n",
            "Pero yo soy tu cuerpo\n",
            "Y te puse la cama todaví\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}